# Clawdbot 商業應用 AI 機器人計劃

> 使用 Clawdbot 建立小規模商業應用的 AI 機器人，評估 Claude Max vs 開源模型（Ollama + RTX 3090）兩種方案

**最後更新**：2026-01-27
**Clawdbot 版本**：v2026.1.24

---

## 📋 專案概述

**目標**：部署一個基於 Clawdbot 的 AI 機器人，用於小規模商業應用（客服、內部助手、自動化工作流等）。

**核心需求**：
- 支援多個訊息平台（LINE、Telegram、Slack 等）
- 穩定且可靠的運行
- 成本可控
- 資料隱私（商業敏感資訊）
- 可擴展性

---

## 🎯 兩種技術方案評估

### 方案 A：Claude Max 帳號（雲端方案）

#### 架構
```
用戶 → Clawdbot (伺服器) → Anthropic Claude API (雲端) → 回應
```

#### 優點
✅ **品質最高**：Claude Opus 4.5 是目前最強的模型
✅ **官方支援**：穩定、持續更新
✅ **OAuth 整合**：Clawdbot 原生支援 Claude Max OAuth
✅ **無需硬體**：不需要 GPU
✅ **快速部署**：安裝即可使用
✅ **多模態支援**：圖片、文件分析

#### 缺點
❌ **月費成本**：Claude Max $20/月（個人）或更高（商業）
❌ **API 限制**：每分鐘請求數、每日用量限制
❌ **依賴網路**：需要穩定外網連線
❌ **資料隱私**：商業敏感資訊傳送到 Anthropic 伺服器
❌ **延遲**：網路往返時間

#### 成本分析（月）
- Claude Max 訂閱：$20 USD（約 NT$ 650）
- 伺服器成本：已有（lt4 或 Auruma）
- **總計：約 NT$ 650/月**

---

### 方案 B：開源模型 + Ollama + RTX 3090（本地方案）

#### 架構
```
用戶 → Clawdbot (伺服器) → Ollama (本地) → RTX 3090 → 開源模型 → 回應
```

#### 優點
✅ **無月費**：僅硬體一次性投資
✅ **無 API 限制**：無限請求數
✅ **資料隱私**：所有資料留在本地
✅ **低延遲**：無需網路往返
✅ **可自訂**：可微調模型
✅ **離線運行**：不依賴外網

#### 缺點
❌ **硬體投資**：RTX 3090（如需購買）約 NT$ 30,000-50,000
❌ **品質較低**：開源模型不如 Claude Opus 4.5
❌ **電費成本**：RTX 3090 功耗約 350W
❌ **維護成本**：需要自己管理、更新模型
❌ **技術門檻**：需要調整參數、優化效能
❌ **散熱與噪音**：需要良好散熱，風扇噪音

#### 硬體需求

**RTX 3090 規格**：
- VRAM：24GB GDDR6X
- CUDA Cores：10,496
- TDP：350W
- 價格：約 NT$ 30,000-50,000（二手）

**伺服器配置需求**：
- CPU：8 核心以上（Auruma 的 Xeon Silver 4110 符合）
- 記憶體：32GB+（Auruma 有 32GB，符合）
- 電源供應器：至少 750W（需確認 Auruma 目前電源）
- PCIe 插槽：x16（需確認 Auruma 主機板）

#### 成本分析

**初期投資**：
- RTX 3090（如需購買）：NT$ 30,000-50,000
- 電源供應器升級（如需要）：NT$ 3,000-5,000
- **總計：NT$ 33,000-55,000**

**運行成本（月）**：
- 電費（24/7 運行）：350W × 24h × 30天 × NT$ 3/度 ≈ NT$ 756
- 伺服器電費（已有）：NT$ 0（已在運行）
- **總計：約 NT$ 756/月**

**投資回收期**：
- 硬體投資 / (方案 A 月費 - 方案 B 月費) = 50,000 / (650 - 756) = **負值（方案 B 月費更高）**
- 但若考慮資料隱私價值、無限 API 使用，長期可能划算

---

### 方案 C：混合方案（推薦）

#### 架構
```
用戶 → Clawdbot (伺服器) ┬→ Claude API (重要任務)
                          └→ Ollama + RTX 3090 (一般任務)
```

#### 優點
✅ **彈性切換**：重要任務用 Claude，一般任務用本地模型
✅ **成本優化**：降低 Claude API 使用量
✅ **資料分級**：敏感資料用本地，一般資料用雲端
✅ **高可用性**：一方故障可切換到另一方
✅ **最佳品質**：關鍵任務使用最強模型

#### 缺點
⚠️ **複雜度高**：需要管理兩套系統
⚠️ **成本累加**：硬體 + 訂閱雙重成本

---

## 🤖 開源模型選擇（方案 B/C）

### 推薦模型（Ollama 支援）

| 模型 | 參數量 | VRAM 需求 | 品質 | 用途 |
|------|--------|-----------|------|------|
| **Llama 3.1 70B** | 70B | ~40GB (量化後 ~22GB) | ⭐⭐⭐⭐ | 通用任務 |
| **Qwen 2.5 72B** | 72B | ~40GB (量化後 ~23GB) | ⭐⭐⭐⭐ | 中文優化 |
| **Mistral Large** | 123B | 需量化到 Q4 | ⭐⭐⭐⭐⭐ | 高品質任務 |
| **DeepSeek V2** | 236B | 需量化到 Q3/Q4 | ⭐⭐⭐⭐ | 程式碼、推理 |
| **Llama 3.1 8B** | 8B | ~6GB | ⭐⭐⭐ | 快速回應 |

**RTX 3090 (24GB) 可運行**：
- ✅ Llama 3.1 70B (Q4 量化，約 22GB)
- ✅ Qwen 2.5 72B (Q4 量化，約 23GB)
- ✅ Llama 3.1 8B (全精度，約 16GB)
- ⚠️ Mistral Large / DeepSeek V2 需要大幅量化（品質下降）

### 中文任務推薦
- **首選**：Qwen 2.5 72B (阿里巴巴，中文優化)
- **備選**：DeepSeek V2 (中國團隊，中文表現佳)

---

## 🔧 Clawdbot v2026.1.24 新功能

### 重要功能（適合商業應用）

1. **Ollama 原生支援**
   - 自動發現本地 Ollama 模型
   - 無需複雜配置

2. **LINE Messaging API 支援**
   - 豐富回覆（Rich Reply）
   - 快速回覆（Quick Reply）
   - 適合台灣商業環境

3. **Telegram 增強**
   - DM 主題作為獨立會話
   - 連結預覽開關

4. **語音功能**
   - Edge TTS（免費，無需 API key）
   - 自動 TTS 模式

5. **執行批准 `/approve`**
   - 跨平台執行批准功能
   - 商業應用的安全控制

### 支援的訊息平台
- ✅ LINE（台灣主流）
- ✅ Telegram
- ✅ Slack（企業常用）
- ✅ Discord
- ✅ Google Chat
- ✅ iMessage
- ✅ Signal
- ✅ Matrix

---

## 📊 商業應用場景分析

### 適合方案 A（Claude Max）的場景

1. **高階客服**
   - 需要精準理解複雜問題
   - 需要專業回答（法律、醫療、技術）
   - 客戶期望高品質

2. **內容創作**
   - 文案撰寫
   - 行銷素材生成
   - 品質要求高

3. **資料分析**
   - 圖表分析
   - 文件摘要
   - 多模態輸入

### 適合方案 B（Ollama）的場景

1. **一般性客服**
   - FAQ 回答
   - 訂單查詢
   - 基本資訊提供

2. **內部助手**
   - 行政流程查詢
   - 會議紀錄整理
   - 資料隱私要求高

3. **自動化工作流**
   - 定時任務
   - 資料處理
   - 通知提醒

### 適合方案 C（混合）的場景

1. **分級服務**
   - 一般客戶 → Ollama
   - VIP 客戶 → Claude

2. **資料敏感度分級**
   - 敏感資訊 → Ollama（本地）
   - 一般資訊 → Claude（雲端）

3. **成本控制**
   - 高峰時段 → Ollama
   - 低峰時段 → Claude

---

## 🗺️ 部署規劃

### 部署位置選擇

#### 選項 1：lt4 (Linode)
- ✅ 已有 Ubuntu 22.04
- ✅ 網路穩定
- ❌ 無 GPU（僅適合方案 A）
- ❌ 記憶體較小（3.8GB，需升級）

#### 選項 2：Auruma Server（推薦方案 B/C）
- ✅ 32GB 記憶體（充足）
- ✅ 8 核心 CPU（充足）
- ✅ 可安裝 RTX 3090（需確認主機板與電源）
- ✅ 本地部署（低延遲）
- ⚠️ 需先遷移到 Linux Ubuntu

#### 選項 3：專用硬體（未來考慮）
- 組建專用 AI 伺服器
- 更好的散熱與擴展性

---

## 🚀 安裝步驟規劃

### Phase 1: 環境準備（2-3 天）

**方案 A（僅 Claude Max）**
- [ ] 選擇部署伺服器（lt4）
- [ ] 安裝 Node.js 22
- [ ] 安裝 Docker

**方案 B（Ollama + RTX 3090）**
- [ ] 確認 Auruma Server 主機板與電源規格
- [ ] 遷移 Auruma 到 Ubuntu 24.04（參考 auruma-linux-migration-pending.md）
- [ ] 安裝 RTX 3090（如需購買，先採購）
- [ ] 安裝 NVIDIA 驅動與 CUDA
- [ ] 安裝 Ollama
- [ ] 下載並測試模型（Qwen 2.5 72B Q4）

**方案 C（混合）**
- [ ] 完成方案 B 的所有步驟
- [ ] 準備 Claude Max 帳號

### Phase 2: Clawdbot 安裝（1 天）

- [ ] Clone Clawdbot v2026.1.24
- [ ] 建立 Docker Compose 配置
- [ ] 設定訊息平台（LINE / Telegram / Slack）
- [ ] 配置 AI 提供者（Claude 或 Ollama）
- [ ] 測試基本功能

### Phase 3: 進階配置（1-2 天）

- [ ] 設定 TTS（Edge TTS 免費方案）
- [ ] 設定執行批准（`/approve`）
- [ ] 設定多平台整合
- [ ] 設定 DM 配對模式（安全性）
- [ ] 設定 webhook 與通知

### Phase 4: 商業功能開發（依需求）

- [ ] 整合現有業務系統（資料庫、CRM）
- [ ] 開發自訂指令（訂單查詢、客戶管理）
- [ ] 設定權限與用戶管理
- [ ] 建立監控與日誌系統
- [ ] 設定備份與災難復原

---

## 💰 成本總結（第一年）

### 方案 A：Claude Max
| 項目 | 金額 |
|------|------|
| Claude Max 訂閱（12 月） | NT$ 7,800 |
| 伺服器（已有 lt4） | NT$ 0 |
| **總計** | **NT$ 7,800** |

### 方案 B：Ollama + RTX 3090
| 項目 | 金額 |
|------|------|
| RTX 3090（一次性） | NT$ 40,000 |
| 電源升級（如需） | NT$ 4,000 |
| 電費（12 月） | NT$ 9,072 |
| **總計** | **NT$ 53,072** |

### 方案 C：混合（推薦）
| 項目 | 金額 |
|------|------|
| RTX 3090 + 電源 | NT$ 44,000 |
| Claude Max（12 月） | NT$ 7,800 |
| 電費（12 月） | NT$ 9,072 |
| **總計** | **NT$ 60,872** |

---

## 🤔 待決定的問題

### 問題 1：選擇哪個方案？

**建議評估因素**：
1. **商業應用規模**：預期每月請求數？
2. **資料敏感度**：是否有商業機密？
3. **品質要求**：客戶對回答品質的期望？
4. **預算**：可接受的年度成本？
5. **技術能力**：團隊是否能維護本地模型？

**初步建議**：
- 🟢 **小規模測試**：方案 A（低成本、快速驗證）
- 🟡 **中規模商用**：方案 C（平衡品質與成本）
- 🔴 **大規模或高隱私**：方案 B（長期成本低、隱私高）

### 問題 2：Auruma Server 硬體是否支援 RTX 3090？

**待確認**：
- [ ] 主機板 PCIe 插槽（需 x16）
- [ ] 電源供應器瓦數（需 750W+）
- [ ] 機箱空間（RTX 3090 長度約 30cm）
- [ ] 散熱空間與風扇配置

**如何確認**：
```bash
# 遠端連線到 Auruma（目前仍是 Windows）
ssh auruma

# 檢查主機板資訊（Windows）
wmic baseboard get product,manufacturer

# 檢查電源（需物理檢查）
```

### 問題 3：訊息平台優先順序？

**候選平台**：
- LINE（台灣商業主流）
- Telegram（技術社群）
- Slack（企業內部）
- 其他

**建議**：先從 LINE 或 Telegram 開始（設定簡單）

### 問題 4：是否需要圖形化管理介面？

Clawdbot 提供 Control UI，但需評估：
- 是否需要對外開放？
- 權限控制需求？
- 是否需要客製化儀表板？

---

## 📅 預計時間軸

### 2026-02

- [ ] **決定採用方案**（A / B / C）
- [ ] 評估 Auruma 硬體相容性（方案 B/C）
- [ ] 採購 RTX 3090（如需要）
- [ ] 準備 Claude Max 帳號（方案 A/C）

### 2026-03

- [ ] 安裝硬體（如方案 B/C）
- [ ] 安裝 Ollama + 模型（如方案 B/C）
- [ ] 安裝 Clawdbot v2026.1.24
- [ ] 設定訊息平台整合
- [ ] 測試與調整

### 2026-04

- [ ] 開發商業功能（整合業務系統）
- [ ] 內部測試
- [ ] 小規模上線（測試用戶）
- [ ] 收集反饋與優化

### 2026-05+

- [ ] 正式上線
- [ ] 監控與維護
- [ ] 功能擴充

---

## 💡 即時想法記錄

### 2026-01-27

- Clawdbot v2026.1.24 原生支援 Ollama，設定簡化
- LINE Messaging API 支援對台灣商業應用很友善
- 方案 C（混合）最有彈性，但複雜度高
- 需先確認 Auruma 硬體是否支援 RTX 3090
- 如果 Auruma 不支援，可考慮外接 eGPU（Thunderbolt）或獨立 GPU 伺服器
- 電費成本不容忽視（24/7 運行，RTX 3090 每月約 NT$ 750）
- Claude Max OAuth 整合簡單，適合快速測試

---

## 📚 參考資料

- Clawdbot GitHub: https://github.com/clawdbot/clawdbot
- Clawdbot v2026.1.24 Release: https://github.com/clawdbot/clawdbot/releases/tag/v2026.1.24
- Ollama 官網: https://ollama.ai
- Qwen 模型: https://qwenlm.github.io
- RTX 3090 規格: https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090-3090ti/

---

**待辦提醒**：
- 定期更新這個計劃
- 追蹤 Clawdbot 新版本
- 評估新的開源模型
- 決定採用方案並執行
- 記錄實際部署經驗
